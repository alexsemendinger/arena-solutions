{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Induction Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy induction is an idea from \"In-Context Learning and Induction Heads\" where you get `[A] [B] ... [A*] [B*]`, where `*` denotes some kind of linguistic similarity.\n",
    "\n",
    "Basically redoing ARENA 1.2: Intro to Mechinterp induction heads experiments with the following modification:\n",
    "\n",
    "\n",
    "Experiment:\n",
    "1. Assemble a collection of synonym or near-synonym pairs -- ideally these are all words that are a single token, for the cleanest version\n",
    "2. Create a random sequence of words, followed by a \"repeated sequence\" of their synonyms.\n",
    "3. Run all of the induction head experiments and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Haven't optimzied this, mostly copied wholesale from ARENA 1.2. Might be able to remove some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "import circuitsvis as cv\n",
    "\n",
    "from plotly_utils import imshow, hist, plot_comp_scores, plot_logit_attribution, plot_loss_difference\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = t.device('mps' if t.backends.mps.is_available() else 'cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(\"using device: \", device)\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading 2L Attn-Only Pretrained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    d_model = 768,\n",
    "    d_head = 64,\n",
    "    n_heads = 12,\n",
    "    n_layers = 2,\n",
    "    n_ctx = 2048,\n",
    "    d_vocab = 50278,\n",
    "    attention_dir = 'causal',\n",
    "    attn_only = 'True',\n",
    "    tokenizer_name = 'EleutherAI/gpt-neox-20b',\n",
    "    seed = 398,\n",
    "    use_attn_result = True,\n",
    "    normalization_type = None,  # default would be 'LN', which is layernorm\n",
    "    positional_embedding_type = 'shortformer' # positional embedding only used for q and k, not for v? apparently makes induction heads more likely?\n",
    ")\n",
    "\n",
    "REPO_ID = \"callummcdougall/attn_only_2L_half\"\n",
    "FILENAME = \"attn_only_2L_half.pth\"\n",
    "\n",
    "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "\n",
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = t.load(weights_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = (\n",
    "        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    )\n",
    "\n",
    "    return log_probs_for_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating list of synonym pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Make a list of all the tokens of the model that are English words\n",
    "2. Feed this list to Claude 3.5 Sonnet and ask for synonym pairs\n",
    "3. Process Claude's list of synonym pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word list\n",
    "with open('./dictionary_large.txt', 'r') as f:\n",
    "    word_set = set(f.read().splitlines())\n",
    "print(f\"loaded list of {len(word_set)} English words\")\n",
    "\n",
    "# get tokens, remove initial spaces and then remove duplicates\n",
    "all_tokens = model.tokenizer.convert_ids_to_tokens(range(model.cfg.d_vocab))\n",
    "word_tokens_with_leading_space = []\n",
    "word_tokens_without_leading_space = []\n",
    "for i, token in enumerate(all_tokens):\n",
    "    if token and token[0] == \"Ä \": \n",
    "        token = token[1:]  # strip leading space\n",
    "        if token in word_set:\n",
    "            word_tokens_with_leading_space.append(token)\n",
    "    elif token in word_set:\n",
    "        word_tokens_without_leading_space.append(token)\n",
    "\n",
    "print(f\"Created lists of {len(word_tokens_with_leading_space)} words with leading space and {len(word_tokens_without_leading_space)} without.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning Claude output into synonym pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asked Claude 3.5 Sonnet to generate synonym pairs\n",
    "with open('./synonym_pairs.txt') as f:\n",
    "    synonym_pair_strings = f.read().splitlines()\n",
    "print(synonym_pair_strings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_leading_space_set = set(word_tokens_with_leading_space)\n",
    "without_leading_space_set = set(word_tokens_without_leading_space)\n",
    "word_tokens_set = with_leading_space_set.union(without_leading_space_set)\n",
    "\n",
    "def token_version(word: str) -> str:\n",
    "    '''Put spaces back in front of words that should have spaces in front'''\n",
    "    if word in word_tokens_with_leading_space:\n",
    "        return ' ' + word\n",
    "    return word\n",
    "\n",
    "synonym_pairs = []\n",
    "for word_pair in synonym_pair_strings:\n",
    "    word1, word2 = word_pair.split(',')\n",
    "    word2 = word2[1:]  # remove leading space (from Claude formatting)\n",
    "    if word1 in word_tokens_set and word2 in word_tokens_set:\n",
    "        synonym_pairs.append( (token_version(word1), token_version(word2)) )\n",
    "print(f\"List of {len(synonym_pairs)} pairs, starting with: \", synonym_pairs[:10])\n",
    "\n",
    "# check that all words are a single token long\n",
    "for word1, word2 in synonym_pairs:\n",
    "    assert len(model.tokenizer.tokenize(word1)) == 1\n",
    "    assert len(model.tokenizer.tokenize(word2)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run and cache model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_synonym_string(\n",
    "#         model: HookedTransformer, seq_len: int, batch: int = 1\n",
    "# ) -> Int[Tensor, \"batch full_seq_len\"]:\n",
    "#     \"\"\"\n",
    "#     Generates a sequence of repeated random tokens.\n",
    "#     Output is rep_tokens: [batch, 1 + 2*seq_len]\n",
    "#     \"\"\"\n",
    "#     d_vocab = model.cfg.d_vocab\n",
    "#     prefix = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long()\n",
    "#     rand_tokens = t.randint(0, d_vocab, size=(batch, seq_len))\n",
    "#     rep_tokens = einops.repeat(rand_tokens, 'batch seq -> batch (2 seq)')\n",
    "#     rep_tokens = t.cat([prefix, rep_tokens], dim=1)\n",
    "#     return rep_tokens\n",
    "\n",
    "# print(model.to_str_tokens(generate_repeated_tokens(model, 5, 2)[1]))\n",
    "\n",
    "# def run_and_cache_model_repeated_tokens(\n",
    "#         model: HookedTransformer, seq_len: int, batch: int = 1\n",
    "# ) -> tuple[Tensor, Tensor, ActivationCache]:\n",
    "#     \"\"\"\n",
    "#     Generates a sequence of repeated random tokens, and runs the model on it, returning logits, tokens and cache\n",
    "\n",
    "#     Should use the `generate_repeated_tokens` function above\n",
    "\n",
    "#     Outputs are:\n",
    "#         rep_tokens: [batch, 1+2*seq_len]\n",
    "#         rep_logits: [batch, 1+2*seq_len, d_vocab]\n",
    "#         rep_cache: The cache of the model run on rep_tokens\n",
    "#     \"\"\"\n",
    "#     tokens = generate_repeated_tokens(model, seq_len, batch).to(device)\n",
    "#     logits, cache = model.run_with_cache(tokens, return_type='logits')\n",
    "#     return tokens, logits, cache\n",
    "    \n",
    "# seq_len = 50\n",
    "# batch = 1\n",
    "# (rep_tokens, rep_logits, rep_cache) = run_and_cache_model_repeated_tokens(model, seq_len, batch)\n",
    "# rep_cache.remove_batch_dim()\n",
    "# rep_str = model.to_str_tokens(rep_tokens)\n",
    "# model.reset_hooks()\n",
    "# log_probs = get_log_probs(rep_logits, rep_tokens).squeeze()\n",
    "\n",
    "# print(f\"Performance on the first half: {log_probs[:seq_len].mean():.3f}\")\n",
    "# print(f\"Performance on the second half: {log_probs[seq_len:].mean():.3f}\")\n",
    "\n",
    "# plot_loss_difference(log_probs, rep_str, seq_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
