{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Induction Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy induction is an idea from \"In-Context Learning and Induction Heads\" where you get `[A] [B] ... [A*] [B*]`, where `*` denotes some kind of linguistic similarity.\n",
    "\n",
    "Basically redoing ARENA 1.2: Intro to Mechinterp induction heads experiments with the following modification:\n",
    "\n",
    "\n",
    "Experiment:\n",
    "1. Assemble a collection of synonym or near-synonym pairs -- ideally these are all words that are a single token, for the cleanest version\n",
    "2. Create a random sequence of words, followed by a \"repeated sequence\" of their synonyms.\n",
    "3. Run all of the induction head experiments and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do / ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas\n",
    "* Try different kinds of pairs -- things that have strong associations or similarities\n",
    "    * country / capital\n",
    "    * object / color\n",
    "    * opposites\n",
    "    * 'some other token with a high cosine similarity'\n",
    "* Rank synonyms by \"strength\" (e.g. \"big / large\" is stronger than \"add / include\")\n",
    "    * Have Sonnet assign a rating to each pair?\n",
    "    * Use some metric intrinsic to the model (cosine similarity?)\n",
    "    * Do \"stronger\" pairs get higher induction scores?\n",
    "* What if words are more than one token long?\n",
    "* Based on tests like above, what kinds of tasks could this model perform in-context? (Or, give higher logprobs than chance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Haven't optimzied this, mostly copied wholesale from ARENA 1.2. Might be able to remove some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "import circuitsvis as cv\n",
    "import random\n",
    "\n",
    "from plotly_utils import imshow, hist, plot_comp_scores, plot_logit_attribution, plot_loss_difference\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = t.device('mps' if t.backends.mps.is_available() else 'cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(\"using device: \", device)\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading 2L Attn-Only Pretrained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    d_model = 768,\n",
    "    d_head = 64,\n",
    "    n_heads = 12,\n",
    "    n_layers = 2,\n",
    "    n_ctx = 2048,\n",
    "    d_vocab = 50278,\n",
    "    attention_dir = 'causal',\n",
    "    attn_only = 'True',\n",
    "    tokenizer_name = 'EleutherAI/gpt-neox-20b',\n",
    "    seed = 398,\n",
    "    use_attn_result = True,\n",
    "    normalization_type = None,  # default would be 'LN', which is layernorm\n",
    "    positional_embedding_type = 'shortformer' # positional embedding only used for q and k, not for v? apparently makes induction heads more likely?\n",
    ")\n",
    "\n",
    "REPO_ID = \"callummcdougall/attn_only_2L_half\"\n",
    "FILENAME = \"attn_only_2L_half.pth\"\n",
    "\n",
    "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "\n",
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = t.load(weights_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = (\n",
    "        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    )\n",
    "\n",
    "    return log_probs_for_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating list of synonym pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Make a list of all the tokens of the model that are English words\n",
    "2. Feed this list to Claude 3.5 Sonnet and ask for synonym pairs\n",
    "3. Process Claude's list of synonym pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word list\n",
    "with open('./dictionary_large.txt', 'r') as f:\n",
    "    word_set = set(f.read().splitlines())\n",
    "print(f\"loaded list of {len(word_set)} English words\")\n",
    "\n",
    "# get tokens, remove initial spaces and then remove duplicates\n",
    "all_tokens = model.tokenizer.convert_ids_to_tokens(range(model.cfg.d_vocab))\n",
    "word_tokens_with_leading_space = []\n",
    "word_tokens_without_leading_space = []\n",
    "for i, token in enumerate(all_tokens):\n",
    "    if token and token[0] == \"Ä \": \n",
    "        token = token[1:]  # strip leading space\n",
    "        if token in word_set:\n",
    "            word_tokens_with_leading_space.append(token)\n",
    "    elif token in word_set:\n",
    "        word_tokens_without_leading_space.append(token)\n",
    "\n",
    "print(f\"Created lists of {len(word_tokens_with_leading_space)} words with leading space and {len(word_tokens_without_leading_space)} without.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning Claude output into synonym pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asked Claude 3.5 Sonnet to generate synonym pairs\n",
    "PAIRS_FILEPATH = './strong_synonym_pairs.txt'\n",
    "\n",
    "with open(PAIRS_FILEPATH) as f:\n",
    "    synonym_pair_strings = f.read().splitlines()\n",
    "print(synonym_pair_strings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_leading_space_set = set(word_tokens_with_leading_space)\n",
    "without_leading_space_set = set(word_tokens_without_leading_space)\n",
    "word_tokens_set = with_leading_space_set.union(without_leading_space_set)\n",
    "\n",
    "def token_version(word: str) -> str:\n",
    "    '''Put spaces back in front of words that should have spaces in front'''\n",
    "    if word in word_tokens_with_leading_space:\n",
    "        return ' ' + word\n",
    "    return word\n",
    "\n",
    "synonym_pairs = []\n",
    "for word_pair in synonym_pair_strings:\n",
    "    word1, word2 = word_pair.split(',')\n",
    "    word2 = word2[1:]  # remove leading space (from Claude formatting)\n",
    "    if word1 in word_tokens_set and word2 in word_tokens_set:\n",
    "        synonym_pairs.append( (token_version(word1), token_version(word2)) )\n",
    "print(f\"List of {len(synonym_pairs)} pairs, starting with: \", synonym_pairs[:10])\n",
    "\n",
    "# check that all words are a single token long\n",
    "for word1, word2 in synonym_pairs:\n",
    "    assert len(model.tokenizer.tokenize(word1)) == 1\n",
    "    assert len(model.tokenizer.tokenize(word2)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run and cache model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fuzzy_tokens(\n",
    "        model: HookedTransformer, \n",
    "        synonym_pairs: list[tuple[str, str]], \n",
    "        seq_len: int, \n",
    "        batch: int = 1\n",
    ") -> Int[Tensor, \"batch full_seq_len\"]:\n",
    "    \"\"\"\n",
    "    Generates a sequence of random tokens followed by their synonyms.\n",
    "    Output is fuzzy_tokens: [batch, 1 + 2*seq_len]\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        synonym_pairs: List of (word1, word2) tuples where words are pre-tokenized\n",
    "        seq_len: Length of the sequence (before repeating with synonyms)\n",
    "        batch: Batch size\n",
    "    \"\"\"\n",
    "    # Start with BOS token\n",
    "    prefix = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long()\n",
    "    \n",
    "    # For each sequence in the batch\n",
    "    all_sequences = []\n",
    "    for b in range(batch):\n",
    "        # Randomly sample seq_len pairs from synonym_pairs\n",
    "        chosen_pairs = random.sample(synonym_pairs, k=seq_len)\n",
    "        \n",
    "        # First half: original words (directly tokenize the pre-formatted strings)\n",
    "        first_half = [model.tokenizer.encode(pair[0], add_special_tokens=False)[0] for pair in chosen_pairs]\n",
    "        # Second half: synonym words\n",
    "        second_half = [model.tokenizer.encode(pair[1], add_special_tokens=False)[0] for pair in chosen_pairs]\n",
    "        \n",
    "        # Combine into one sequence\n",
    "        sequence = first_half + second_half\n",
    "        all_sequences.append(sequence)\n",
    "    \n",
    "    # Convert to tensor [batch, 2*seq_len]\n",
    "    fuzzy_tokens = t.tensor(all_sequences).long()\n",
    "    # Add prefix [batch, 1 + 2*seq_len]\n",
    "    fuzzy_tokens = t.cat([prefix, fuzzy_tokens], dim=1)\n",
    "    return fuzzy_tokens\n",
    "\n",
    "def run_and_cache_model_fuzzy_tokens(\n",
    "        model: HookedTransformer, \n",
    "        synonym_pairs: list[tuple[str, str]], \n",
    "        seq_len: int, \n",
    "        batch: int = 1\n",
    ") -> tuple[Tensor, Tensor, ActivationCache]:\n",
    "    \"\"\"\n",
    "    Generates a sequence of random tokens followed by their synonyms, and runs the model on it.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        synonym_pairs: List of (word1, word2) tuples where words are pre-tokenized\n",
    "        seq_len: Length of the sequence (before repeating with synonyms)\n",
    "        batch: Batch size\n",
    "    \n",
    "    Returns:\n",
    "        fuzzy_tokens: [batch, 1+2*seq_len]\n",
    "        fuzzy_logits: [batch, 1+2*seq_len, d_vocab]\n",
    "        fuzzy_cache: The cache of the model run on fuzzy_tokens\n",
    "    \"\"\"\n",
    "    tokens = generate_fuzzy_tokens(model, synonym_pairs, seq_len, batch).to(device)\n",
    "    logits, cache = model.run_with_cache(tokens, return_type='logits')\n",
    "    return tokens, logits, cache\n",
    "\n",
    "# Example usage and testing:\n",
    "def test_fuzzy_tokens(model, synonym_pairs, seq_len=5, batch=1):\n",
    "    \"\"\"Helper function to test and visualize the token generation\"\"\"\n",
    "    tokens = generate_fuzzy_tokens(model, synonym_pairs, seq_len, batch)\n",
    "    print(\"Generated sequence:\")\n",
    "    print(model.to_str_tokens(tokens[0]))  # Show first batch\n",
    "    print(\"\\nFirst half (original words):\", model.to_str_tokens(tokens[0][1:seq_len+1]))\n",
    "    print(\"Second half (synonyms):\", model.to_str_tokens(tokens[0][seq_len+1:]))\n",
    "    return tokens\n",
    "\n",
    "# Main experiment:\n",
    "seq_len = 50\n",
    "batch = 1\n",
    "(fuzzy_tokens, fuzzy_logits, fuzzy_cache) = run_and_cache_model_fuzzy_tokens(\n",
    "    model, synonym_pairs, seq_len, batch\n",
    ")\n",
    "fuzzy_cache.remove_batch_dim()\n",
    "fuzzy_str = model.to_str_tokens(fuzzy_tokens)\n",
    "model.reset_hooks()\n",
    "log_probs = get_log_probs(fuzzy_logits, fuzzy_tokens).squeeze()\n",
    "\n",
    "print(\"Tokens: \", end='')\n",
    "for i in range(seq_len):\n",
    "    print(f'({fuzzy_str[1:seq_len+1][i]}, {fuzzy_str[seq_len+1:][i]})', end=' ')\n",
    "print('\\n')\n",
    "\n",
    "print(f\"Performance on the first half (original words): {log_probs[:seq_len].mean():.3f}\")\n",
    "print(f\"Performance on the second half (synonyms): {log_probs[seq_len:].mean():.3f}\")\n",
    "\n",
    "plot_loss_difference(log_probs, fuzzy_str, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attention heads\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    attention_pattern = fuzzy_cache[\"pattern\", layer]\n",
    "\n",
    "    print(f\"Layer {layer} Head Attn Patterns\")\n",
    "    display(cv.attention.attention_patterns(\n",
    "        tokens = fuzzy_str,\n",
    "        attention = attention_pattern,\n",
    "        attention_head_names = [f'Layer {layer}, Head {i}' for i in range(model.cfg.n_heads)]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much weaker than in the strict repeated-token case, but we see a *slight* reduction in loss on the repeated sequence, and we see a definite (although faint) induction stripe in heads 1.4 and 1.10!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate induction scores with hooks\n",
    "seq_len = 50\n",
    "batch = 10\n",
    "fuzzy_tokens_10 = generate_fuzzy_tokens(model, synonym_pairs, seq_len, batch)\n",
    "\n",
    "# Store the induction score for each head.\n",
    "induction_score_store = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "\n",
    "def induction_score_hook(\n",
    "    pattern: Float[Tensor, \"batch head_index dest_pos source_pos\"],\n",
    "    hook: HookPoint,\n",
    "):\n",
    "    seq_len = (pattern.shape[-1] - 1) // 2\n",
    "    induction_scores = einops.reduce(t.diagonal(pattern, offset=-seq_len+1, dim1=-2, dim2=-1), \"batch head pos -> head\", \"mean\")\n",
    "    induction_score_store[hook.layer()] = induction_scores\n",
    "\n",
    "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "\n",
    "# Run with hooks (this is where we write to the `induction_score_store` tensor`)\n",
    "model.run_with_hooks(\n",
    "    fuzzy_tokens_10, \n",
    "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "    fwd_hooks=[(\n",
    "        pattern_hook_names_filter,\n",
    "        induction_score_hook  \n",
    "    )]\n",
    ")\n",
    "\n",
    "# Plot the induction scores for each head in each layer\n",
    "imshow(\n",
    "    induction_score_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Induction Score by Head\", \n",
    "    text_auto=\".2f\",\n",
    "    width=900, height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heads 1.4 (score 0.06 / previous score 0.66) and 1.10 (score 0.17 / previous score 0.84) stand out as induction heads again by this metric, but are far weaker than in the \"pure repetition\" case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablate positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_ablate_hook(pattern, hook):\n",
    "    return t.zeros_like(pattern)\n",
    "\n",
    "pos_embed_filter = lambda name: \"pos_embed\" in name\n",
    "\n",
    "model.add_hook(pos_embed_filter, zero_ablate_hook, 'fwd')\n",
    "model.add_hook(pattern_hook_names_filter, induction_score_hook, 'fwd')\n",
    "\n",
    "# Main experiment:\n",
    "seq_len = 50\n",
    "batch = 1\n",
    "(fuzzy_tokens, fuzzy_logits, fuzzy_cache) = run_and_cache_model_fuzzy_tokens(\n",
    "    model, synonym_pairs, seq_len, batch\n",
    ")\n",
    "fuzzy_cache.remove_batch_dim()\n",
    "fuzzy_str = model.to_str_tokens(fuzzy_tokens)\n",
    "model.reset_hooks()\n",
    "log_probs = get_log_probs(fuzzy_logits, fuzzy_tokens).squeeze()\n",
    "model.remove_all_hook_fns()\n",
    "\n",
    "print(\"Tokens: \", end='')\n",
    "for i in range(seq_len):\n",
    "    print(f'({fuzzy_str[1:seq_len+1][i]}, {fuzzy_str[seq_len+1:][i]})', end=' ')\n",
    "print('\\n')\n",
    "\n",
    "print(f\"Performance on the first half (original words): {log_probs[:seq_len].mean():.3f}\")\n",
    "print(f\"Performance on the second half (synonyms): {log_probs[seq_len:].mean():.3f}\")\n",
    "\n",
    "plot_loss_difference(log_probs, fuzzy_str, seq_len)\n",
    "\n",
    "# visualize attention heads\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    attention_pattern = fuzzy_cache[\"pattern\", layer]\n",
    "\n",
    "    print(f\"Layer {layer} Head Attn Patterns\")\n",
    "    display(cv.attention.attention_patterns(\n",
    "        tokens = fuzzy_str,\n",
    "        attention = attention_pattern,\n",
    "        attention_head_names = [f'Layer {layer}, Head {i}' for i in range(model.cfg.n_heads)]\n",
    "    ))\n",
    "\n",
    "imshow(\n",
    "    induction_score_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Induction scores after zero-ablating positional encoding\",\n",
    "    range_color=(-1,1),\n",
    "    text_auto=\".2f\",\n",
    "    width=900, height=400\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
