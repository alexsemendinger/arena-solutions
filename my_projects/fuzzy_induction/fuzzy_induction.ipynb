{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Induction Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy induction is an idea from \"In-Context Learning and Induction Heads\" where you get `[A] [B] ... [A*] [B*]`, where `*` denotes some kind of linguistic similarity.\n",
    "\n",
    "Basically redoing ARENA 1.2: Intro to Mechinterp induction heads experiments with the following modification:\n",
    "\n",
    "\n",
    "Experiment:\n",
    "1. Assemble a collection of synonym or near-synonym pairs -- ideally these are all words that are a single token, for the cleanest version\n",
    "2. Create a random sequence of words, followed by a \"repeated sequence\" of their synonyms.\n",
    "3. Run all of the induction head experiments and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Haven't optimzied this, mostly copied wholesale from ARENA 1.2. Might be able to remove some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "import circuitsvis as cv\n",
    "\n",
    "from plotly_utils import imshow, hist, plot_comp_scores, plot_logit_attribution, plot_loss_difference\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = t.device('mps' if t.backends.mps.is_available() else 'cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(\"using device: \", device)\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading 2L Attn-Only Pretrained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    d_model = 768,\n",
    "    d_head = 64,\n",
    "    n_heads = 12,\n",
    "    n_layers = 2,\n",
    "    n_ctx = 2048,\n",
    "    d_vocab = 50278,\n",
    "    attention_dir = 'causal',\n",
    "    attn_only = 'True',\n",
    "    tokenizer_name = 'EleutherAI/gpt-neox-20b',\n",
    "    seed = 398,\n",
    "    use_attn_result = True,\n",
    "    normalization_type = None,  # default would be 'LN', which is layernorm\n",
    "    positional_embedding_type = 'shortformer' # positional embedding only used for q and k, not for v? apparently makes induction heads more likely?\n",
    ")\n",
    "\n",
    "REPO_ID = \"callummcdougall/attn_only_2L_half\"\n",
    "FILENAME = \"attn_only_2L_half.pth\"\n",
    "\n",
    "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "\n",
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = t.load(weights_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating list of synonym pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Make a list of all the tokens of the model that are English words\n",
    "2. Feed this list to Claude 3.5 Sonnet and ask for synonym pairs\n",
    "3. Process Claude's list of synonym pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded list of 181107 English words\n",
      "List of 16164 word tokens created.\n"
     ]
    }
   ],
   "source": [
    "# get tokens, remove initial spaces and then remove duplicates\n",
    "all_tokens = model.tokenizer.convert_ids_to_tokens(range(model.cfg.d_vocab))\n",
    "tokens_with_initial_space = []\n",
    "for i, token in enumerate(all_tokens):\n",
    "    if token and token[0] == \"Ä \":\n",
    "        tokens_with_initial_space.append(token)\n",
    "        all_tokens[i] = token[1:]  # remove initial spaces\n",
    "all_tokens_set = set(all_tokens)\n",
    "\n",
    "# load word list\n",
    "with open('./dictionary_large.txt', 'r') as f:\n",
    "    word_set = set(f.read().splitlines())\n",
    "print(f\"loaded list of {len(word_set)} English words\")\n",
    "\n",
    "# take intersection to get set, list of word tokens\n",
    "word_tokens_set = all_tokens_set.intersection(word_set)\n",
    "word_tokens = list(word_tokens_set)\n",
    "print(f\"List of {len(word_tokens)} word tokens created.\")\n",
    "\n",
    "# write to file (don't need to do this again)\n",
    "# with open('./word_tokens.txt', 'w') as f:\n",
    "#     f.write(\"\\n\".join(word_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning Claude output into synonym pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accidental, unintended', 'accurate, precise', 'add, include', 'advance, progress', 'afraid, scared', 'allow, permit', 'already, previously', 'also, too', 'always, constantly', 'ancient, old']\n"
     ]
    }
   ],
   "source": [
    "# Asked Claude 3.5 Sonnet to generate synonym pairs\n",
    "with open('./synonym_pairs.txt') as f:\n",
    "    synonym_pair_strings = f.read().splitlines()\n",
    "print(synonym_pair_strings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 848 pairs, starting with:  [('accidental', 'unintended'), ('accurate', 'precise'), ('add', 'include'), ('advance', 'progress'), ('afraid', 'scared'), ('allow', 'permit'), ('already', 'previously'), ('also', 'too'), ('always', 'constantly'), ('ancient', 'old')]\n"
     ]
    }
   ],
   "source": [
    "synonym_pairs = []\n",
    "for word_pair in synonym_pair_strings:\n",
    "    word1, word2 = word_pair.split(',')\n",
    "    word2 = word2[1:]  # remove leading space\n",
    "    if word1 in word_tokens_set and word2 in word_tokens_set:\n",
    "        synonym_pairs.append( (word1, word2) )\n",
    "print(f\"List of {len(synonym_pairs)} pairs, starting with: \", synonym_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run and cache model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_repeated_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     rep_tokens \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcat([prefix, rep_tokens], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rep_tokens\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mto_str_tokens(\u001b[43mgenerate_repeated_tokens\u001b[49m(model, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_and_cache_model_repeated_tokens\u001b[39m(\n\u001b[1;32m     20\u001b[0m         model: HookedTransformer, seq_len: \u001b[38;5;28mint\u001b[39m, batch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor, ActivationCache]:\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    Generates a sequence of repeated random tokens, and runs the model on it, returning logits, tokens and cache\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m        rep_cache: The cache of the model run on rep_tokens\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_repeated_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: modify this (from ARENA 1.2) for current task\n",
    "\n",
    "# def generate_synonym_string(\n",
    "#         model: HookedTransformer, seq_len: int, batch: int = 1\n",
    "# ) -> Int[Tensor, \"batch full_seq_len\"]:\n",
    "#     \"\"\"\n",
    "#     Generates a sequence of repeated random tokens.\n",
    "#     Output is rep_tokens: [batch, 1 + 2*seq_len]\n",
    "#     \"\"\"\n",
    "#     d_vocab = model.cfg.d_vocab\n",
    "#     prefix = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long()\n",
    "#     rand_tokens = t.randint(0, d_vocab, size=(batch, seq_len))\n",
    "#     rep_tokens = einops.repeat(rand_tokens, 'batch seq -> batch (2 seq)')\n",
    "#     rep_tokens = t.cat([prefix, rep_tokens], dim=1)\n",
    "#     return rep_tokens\n",
    "\n",
    "# print(model.to_str_tokens(generate_repeated_tokens(model, 5, 2)[1]))\n",
    "\n",
    "# def run_and_cache_model_repeated_tokens(\n",
    "#         model: HookedTransformer, seq_len: int, batch: int = 1\n",
    "# ) -> tuple[Tensor, Tensor, ActivationCache]:\n",
    "#     \"\"\"\n",
    "#     Generates a sequence of repeated random tokens, and runs the model on it, returning logits, tokens and cache\n",
    "\n",
    "#     Should use the `generate_repeated_tokens` function above\n",
    "\n",
    "#     Outputs are:\n",
    "#         rep_tokens: [batch, 1+2*seq_len]\n",
    "#         rep_logits: [batch, 1+2*seq_len, d_vocab]\n",
    "#         rep_cache: The cache of the model run on rep_tokens\n",
    "#     \"\"\"\n",
    "#     tokens = generate_repeated_tokens(model, seq_len, batch).to(device)\n",
    "#     logits, cache = model.run_with_cache(tokens, return_type='logits')\n",
    "#     return tokens, logits, cache\n",
    "    \n",
    "# seq_len = 50\n",
    "# batch = 1\n",
    "# (rep_tokens, rep_logits, rep_cache) = run_and_cache_model_repeated_tokens(model, seq_len, batch)\n",
    "# rep_cache.remove_batch_dim()\n",
    "# rep_str = model.to_str_tokens(rep_tokens)\n",
    "# model.reset_hooks()\n",
    "# log_probs = get_log_probs(rep_logits, rep_tokens).squeeze()\n",
    "\n",
    "# print(f\"Performance on the first half: {log_probs[:seq_len].mean():.3f}\")\n",
    "# print(f\"Performance on the second half: {log_probs[seq_len:].mean():.3f}\")\n",
    "\n",
    "# plot_loss_difference(log_probs, rep_str, seq_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
