{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARENA 1.2: Intro to Mechinterp\n",
    "\n",
    "Alex Semendinger\n",
    "\n",
    "October 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Code is executing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "import circuitsvis as cv\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformer_interp\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part2_intro_to_mech_interp\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from plotly_utils import imshow, hist, plot_comp_scores, plot_logit_attribution, plot_loss_difference\n",
    "from part1_transformer_from_scratch.solutions import get_log_probs\n",
    "import part2_intro_to_mech_interp.tests as tests\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = t.device('mps' if t.backends.mps.is_available() else 'cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(\"using device: \", device)\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Intro to TransformerLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and running models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_small: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Facts about GPT-2 Small model\")\n",
    "print(\" n_layers:\", gpt_small.cfg.n_layers)\n",
    "print(\" n heads per layer: \", gpt_small.cfg.n_heads)\n",
    "print(\" maximum context window: \", gpt_small.cfg.n_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_small = gpt_small\n",
    "# model_description_text = '''## Loading Models\n",
    "\n",
    "# HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.\n",
    "\n",
    "# For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!'''\n",
    "\n",
    "model_description_text = \"The the the the the the the the the\"\n",
    "\n",
    "loss = gpt2_small(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt2_small.to_str_tokens(\"gpt2\"))\n",
    "print(gpt2_small.to_str_tokens([\"gpt2\", \"gpt2\"]))\n",
    "print(gpt2_small.to_tokens(\"gpt2\"))\n",
    "print(gpt2_small.to_string([50256, 70, 457, 17]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: how many tokens does the model guess correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits: Tensor = gpt2_small(model_description_text, return_type=\"logits\")\n",
    "prediction = logits.argmax(dim=-1).squeeze()[:-1]\n",
    "true_tokens = gpt2_small.to_tokens(model_description_text).squeeze()[1:]\n",
    "n_correct = (true_tokens == prediction).sum()\n",
    "print(f'Model answered {n_correct}/{len(true_tokens)} correctly')\n",
    "print(f'Correct tokens: {gpt2_small.to_string(prediction[prediction == true_tokens])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching all activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "#gpt2_text = \"this is a short text\"\n",
    "gpt2_text = \"My entrepreneurship responsibilities are straightforward: to enthusiastically provide telecommunications and cryptocurrencies to disproportionately affected environmentalists in Charlottesville\"\n",
    "\n",
    "gpt2_tokens = gpt2_small.to_tokens(gpt2_text)\n",
    "gpt2_logits, gpt2_cache = gpt2_small.run_with_cache(gpt2_tokens, remove_batch_dim=True)\n",
    "\n",
    "#print(f'gpt2_cache contains the following keys: {gpt2_cache.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note multiple ways of accessing the same item\n",
    "attn_patterns_layer_0 = gpt2_cache[\"pattern\", 0]\n",
    "also_attn_patterns_layer_0 = gpt2_cache[\"blocks.0.attn.hook_pattern\"]\n",
    "t.testing.assert_close(attn_patterns_layer_0, also_attn_patterns_layer_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: verify activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0_pattern_from_cache = gpt2_cache[\"pattern\", 0]  # attention pattern\n",
    "\n",
    "layer0_q = gpt2_cache[\"q\", 0]  # size (n_tokens, n_heads, d_head), which is (33, 12, 64)\n",
    "layer0_k = gpt2_cache[\"k\", 0]\n",
    "n_tokens = layer0_q.shape[0]\n",
    "\n",
    "# compute scaled dot product, mask, softmax\n",
    "layer0_attn_scores_from_q_and_k = einops.einsum(layer0_q, layer0_k, 'seqQ nhead dhead, seqK nhead dhead -> nhead seqQ seqK') / gpt2_small.cfg.d_head ** 0.5\n",
    "layer0_attn_scores_from_q_and_k.masked_fill_(mask=t.triu(t.ones((n_tokens, n_tokens)), diagonal=1).to(device).bool(),\n",
    "                                             value=float('-inf'))\n",
    "layer0_pattern_from_q_and_k = layer0_attn_scores_from_q_and_k.softmax(dim=-1)\n",
    "\n",
    "t.testing.assert_close(layer0_pattern_from_cache, layer0_pattern_from_q_and_k)\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gpt2_cache))\n",
    "attention_pattern = gpt2_cache[\"pattern\", 0]\n",
    "print(attention_pattern.shape)\n",
    "gpt2_str_tokens = gpt2_small.to_str_tokens(gpt2_text)\n",
    "\n",
    "print(\"Layer 0 Head Attn Patterns\")\n",
    "display(cv.attention.attention_patterns(\n",
    "    tokens = gpt2_str_tokens,\n",
    "    attention = attention_pattern,\n",
    "    attention_head_names = [f'LOH{i}' for i in range(gpt2_small.cfg.n_heads)]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Finding Induction Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing toy attention-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    d_model = 768,\n",
    "    d_head = 64,\n",
    "    n_heads = 12,\n",
    "    n_layers = 2,\n",
    "    n_ctx = 2048,\n",
    "    d_vocab = 50278,\n",
    "    attention_dir = 'causal',\n",
    "    attn_only = 'True',\n",
    "    tokenizer_name = 'EleutherAI/gpt-neox-20b',\n",
    "    seed = 398,\n",
    "    use_attn_result = True,\n",
    "    normalization_type = None,  # default would be 'LN', which is layernorm\n",
    "    positional_embedding_type = 'shortformer' # positional embedding only used for q and k, not for v? apparently makes induction heads more likely?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO_ID = \"callummcdougall/attn_only_2L_half\"\n",
    "FILENAME = \"attn_only_2L_half.pth\"\n",
    "\n",
    "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = t.load(weights_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: visualize attention patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this.\"\n",
    "text = \"hogrning. i like hogrning. we all enjoy hogrning. sometimes hogrning happens in spring. hogrning is fun.\"\n",
    "\n",
    "logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "str_tokens = model.to_str_tokens(text)\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    attention_pattern = cache[\"pattern\", layer]\n",
    "\n",
    "    print(f\"Layer {layer} Head Attn Patterns\")\n",
    "    display(cv.attention.attention_patterns(\n",
    "        tokens = str_tokens,\n",
    "        attention = attention_pattern,\n",
    "        attention_head_names = [f'Layer {layer}, Head {i}' for i in range(model.cfg.n_heads)]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits= model(text, return_type=\"logits\")\n",
    "loss = model(text, return_type=\"loss\")\n",
    "prediction = logits.argmax(dim=-1).squeeze()[:-1]\n",
    "true_tokens = model.to_tokens(text).squeeze()[1:]\n",
    "n_correct = (true_tokens == prediction).sum()\n",
    "print(f'Model answered {n_correct}/{len(true_tokens)} correctly')\n",
    "print(f'Correct tokens: {model.to_str_tokens(prediction[prediction == true_tokens])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: write your own detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_attn_detector(cache: ActivationCache) -> list[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be current-token heads\n",
    "    '''\n",
    "    PROB_THRESHOLD = 0.3\n",
    "    heads = []\n",
    "    for layer in range(2):\n",
    "        pattern = cache[\"pattern\", layer]\n",
    "        avg_current_token_probs = t.diagonal(pattern, dim1=1, dim2=2).mean(dim=-1)\n",
    "        head_indices = t.nonzero(avg_current_token_probs > PROB_THRESHOLD)\n",
    "        heads.extend([f'{layer}.{idx.item()}' for idx in head_indices])\n",
    "        #heads = dict(zip(heads, (a.item() for a in avg_current_token_probs[head_indices])))\n",
    "    return heads\n",
    "    \n",
    "\n",
    "def prev_attn_detector(cache: ActivationCache) -> list[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
    "    '''\n",
    "    PROB_THRESHOLD = 0.35\n",
    "    heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        pattern = cache[\"pattern\", layer]\n",
    "        avg_prev_token_probs = t.diagonal(pattern, offset=-1, dim1=1, dim2=2).mean(dim=-1)\n",
    "        head_indices = t.nonzero(avg_prev_token_probs > PROB_THRESHOLD)\n",
    "        heads.extend([f'{layer}.{idx.item()}' for idx in head_indices])\n",
    "    return heads\n",
    "\n",
    "def first_attn_detector(cache: ActivationCache) -> list[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
    "    '''\n",
    "    PROB_THRESHOLD = 0.7\n",
    "    heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        pattern = cache[\"pattern\", layer]\n",
    "        avg_first_token_probs = pattern[:, :, 0].mean(dim=-1)\n",
    "        head_indices = t.nonzero(avg_first_token_probs > PROB_THRESHOLD)\n",
    "        heads.extend([f'{layer}.{idx.item()}' for idx in head_indices])\n",
    "    return heads\n",
    "\n",
    "\n",
    "print(\"Heads attending to current token  = \", \", \".join(current_attn_detector(cache)))\n",
    "print(\"Heads attending to previous token = \", \", \".join(prev_attn_detector(cache)))\n",
    "print(\"Heads attending to first token    = \", \", \".join(first_attn_detector(cache)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding induction heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise: plot per-token loss on repeated sequence\n",
    "\n",
    "def generate_repeated_tokens(\n",
    "        model: HookedTransformer, seq_len: int, batch: int = 1\n",
    ") -> Int[Tensor, \"batch full_seq_len\"]:\n",
    "    \"\"\"\n",
    "    Generates a sequence of repeated random tokens.\n",
    "    Output is rep_tokens: [batch, 1 + 2*seq_len]\n",
    "    \"\"\"\n",
    "    d_vocab = model.cfg.d_vocab\n",
    "    prefix = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long()\n",
    "    rand_tokens = t.randint(0, d_vocab, size=(batch, seq_len))\n",
    "    rep_tokens = einops.repeat(rand_tokens, 'batch seq -> batch (2 seq)')\n",
    "    rep_tokens = t.cat([prefix, rep_tokens], dim=1)\n",
    "    return rep_tokens\n",
    "\n",
    "print(model.to_str_tokens(generate_repeated_tokens(model, 5, 2)[1]))\n",
    "\n",
    "def run_and_cache_model_repeated_tokens(\n",
    "        model: HookedTransformer, seq_len: int, batch: int = 1\n",
    ") -> tuple[Tensor, Tensor, ActivationCache]:\n",
    "    \"\"\"\n",
    "    Generates a sequence of repeated random tokens, and runs the model on it, returning logits, tokens and cache\n",
    "\n",
    "    Should use the `generate_repeated_tokens` function above\n",
    "\n",
    "    Outputs are:\n",
    "        rep_tokens: [batch, 1+2*seq_len]\n",
    "        rep_logits: [batch, 1+2*seq_len, d_vocab]\n",
    "        rep_cache: The cache of the model run on rep_tokens\n",
    "    \"\"\"\n",
    "    tokens = generate_repeated_tokens(model, seq_len, batch).to(device)\n",
    "    logits, cache = model.run_with_cache(tokens, return_type='logits')\n",
    "    return tokens, logits, cache\n",
    "    \n",
    "seq_len = 50\n",
    "batch = 1\n",
    "(rep_tokens, rep_logits, rep_cache) = run_and_cache_model_repeated_tokens(model, seq_len, batch)\n",
    "rep_cache.remove_batch_dim()\n",
    "rep_str = model.to_str_tokens(rep_tokens)\n",
    "model.reset_hooks()\n",
    "log_probs = get_log_probs(rep_logits, rep_tokens).squeeze()\n",
    "\n",
    "print(f\"Performance on the first half: {log_probs[:seq_len].mean():.3f}\")\n",
    "print(f\"Performance on the second half: {log_probs[seq_len:].mean():.3f}\")\n",
    "\n",
    "plot_loss_difference(log_probs, rep_str, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize attention heads\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    attention_pattern = rep_cache[\"pattern\", layer]\n",
    "\n",
    "    print(f\"Layer {layer} Head Attn Patterns\")\n",
    "    display(cv.attention.attention_patterns(\n",
    "        tokens = rep_str,\n",
    "        attention = attention_pattern,\n",
    "        attention_head_names = [f'Layer {layer}, Head {i}' for i in range(model.cfg.n_heads)]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def induction_attn_detector(cache: ActivationCache) -> list[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be induction heads\n",
    "    '''\n",
    "    PROB_THRESHOLD = 0.5\n",
    "    heads = []\n",
    "\n",
    "    seq_len = (cache[\"pattern\", 0].shape[-1] - 1) // 2\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        pattern = cache[\"pattern\", layer]\n",
    "        avg_prev_token_probs = t.diagonal(pattern, offset=-seq_len+1, dim1=1, dim2=2).mean(dim=-1)\n",
    "        head_indices = t.nonzero(avg_prev_token_probs > PROB_THRESHOLD)\n",
    "        heads.extend([f'{layer}.{idx.item()}' for idx in head_indices])\n",
    "    return heads\n",
    "\n",
    "print(\"Induction heads = \", \", \".join(induction_attn_detector(rep_cache)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Hooks in TransformerLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooks let us *edit* and *intervene on* activations anywhere in the model.\n",
    "\n",
    "TransformerLens includes a `model.run_with_hooks` function that lets you specify **hook functions** to use on activations.\n",
    "\n",
    "A hook function takes two arguments: `activation_value` (the activation in the model, like what's stored in an `ActivationCache`) and `hook_point` (an object that gives us methods like `hook.layer()` and `hook.name` -- I guess we'll see this later). It should either return nothing (if we're e.g. just looking at the value and doing some processing) or should return a tensor of the same size as the input (if we're editing the value).\n",
    "\n",
    "```\n",
    "# Template for a hook function (this one is currently a null-op)\n",
    "\n",
    "def hook_function(\n",
    "        attn_pattern: Float[Tensor, \"batch heads seqQ seqK\"],\n",
    "        hook: HookPoint\n",
    ") -> Float[Tensor, \"batch heads seqQ seqK\"]:\n",
    "    \n",
    "    # modify attention pattern (can be in-place)\n",
    "    return attn_pattern\n",
    "\n",
    "\n",
    "# Template for calling `model.run_with_hooks`\n",
    "loss = model.run_with_hooks(\n",
    "    tokens,\n",
    "    return_type = \"loss\",\n",
    "    fwd_hooks = [\n",
    "        ('blocks.1.attn.hook_pattern', hook_function)\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate induction scores with hooks\n",
    "seq_len = 50\n",
    "batch = 10\n",
    "rep_tokens_10 = generate_repeated_tokens(model, seq_len, batch)\n",
    "\n",
    "# Store the induction score for each head.\n",
    "induction_score_store = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "\n",
    "def induction_score_hook(\n",
    "    pattern: Float[Tensor, \"batch head_index dest_pos source_pos\"],\n",
    "    hook: HookPoint,\n",
    "):\n",
    "    seq_len = (pattern.shape[-1] - 1) // 2\n",
    "    induction_scores = einops.reduce(t.diagonal(pattern, offset=-seq_len+1, dim1=-2, dim2=-1), \"batch head pos -> head\", \"mean\")\n",
    "    induction_score_store[hook.layer()] = induction_scores\n",
    "\n",
    "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "\n",
    "# Run with hooks (this is where we write to the `induction_score_store` tensor`)\n",
    "model.run_with_hooks(\n",
    "    rep_tokens_10, \n",
    "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "    fwd_hooks=[(\n",
    "        pattern_hook_names_filter,\n",
    "        induction_score_hook\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Plot the induction scores for each head in each layer\n",
    "imshow(\n",
    "    induction_score_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Induction Score by Head\", \n",
    "    text_auto=\".2f\",\n",
    "    width=900, height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: finding induction heads in GPT2-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pattern_hook(\n",
    "    pattern: Float[Tensor, \"batch head_index dest_pos source_pos\"],\n",
    "    hook: HookPoint,\n",
    "):\n",
    "    print(\"Layer: \", hook.layer())\n",
    "    display(\n",
    "        cv.attention.attention_patterns(\n",
    "            tokens=gpt2_small.to_str_tokens(rep_tokens[0]), \n",
    "            attention=pattern.mean(0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize induction score of each head in gpt2_small\n",
    "\n",
    "seq_len = 50\n",
    "batch = 10\n",
    "rep_tokens_10 = generate_repeated_tokens(gpt2_small, seq_len, batch)\n",
    "\n",
    "induction_score_store = t.zeros((gpt2_small.cfg.n_layers, gpt2_small.cfg.n_heads), device=gpt2_small.cfg.device)\n",
    "\n",
    "gpt2_small.run_with_hooks(\n",
    "    rep_tokens_10,\n",
    "    return_type = None,\n",
    "    fwd_hooks = [(pattern_hook_names_filter, induction_score_hook)]\n",
    ")\n",
    "\n",
    "# Plot the induction scores for each head in each layer\n",
    "imshow(\n",
    "    induction_score_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Induction Score by Head in gpt2_small\", \n",
    "    text_auto=\".2f\",\n",
    "    range_color=(-1,1),\n",
    "    width=500, height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My quick experiment: ablate positional encoding to destroy induction heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_ablate_hook(pattern, hook):\n",
    "    return t.zeros_like(pattern)\n",
    "\n",
    "pos_embed_filter = lambda name: \"pos_embed\" in name\n",
    "\n",
    "gpt2_small.run_with_hooks(\n",
    "    rep_tokens_10,\n",
    "    return_type=None,\n",
    "    fwd_hooks = [\n",
    "        (pattern_hook_names_filter, induction_score_hook),\n",
    "        (pos_embed_filter, zero_ablate_hook)\n",
    "    ]\n",
    ")\n",
    "\n",
    "imshow(\n",
    "    induction_score_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Induction Scores after zero-ablating positional encoding\",\n",
    "    range_color=(-1,1),\n",
    "    text_auto=\".2f\",\n",
    "    width=500, height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at attention patterns of heads with high induction scores\n",
    "\n",
    "# # hacky not great way to do this filter\n",
    "# suspected_induction_heads_filter = lambda name: name.endswith(\"pattern\") and name[7] in {\"5\", \"6\", \"7\"}\n",
    "\n",
    "# gpt2_small.run_with_hooks(\n",
    "#     rep_tokens_10,\n",
    "#     return_type = None,\n",
    "#     fwd_hooks = [(suspected_induction_heads_filter, visualize_pattern_hook)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building interpretability tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit attribution tool\n",
    "\n",
    "def logit_attribution(\n",
    "        embed: Float[Tensor, \"seq d_model\"],\n",
    "        l1_results: Float[Tensor, \"seq nheads d_model\"],\n",
    "        l2_results: Float[Tensor, \"seq nheads d_model\"],\n",
    "        W_U: Float[Tensor, \"d_model d_vocab\"],\n",
    "        tokens: Int[Tensor, \"seq-1 n_components\"]\n",
    ") -> Float[Tensor, \"seq-1 n_components\"]:\n",
    "    '''\n",
    "    Inputs:\n",
    "        embed: the embeddings of the tokens (i.e. token + position embeddings)\n",
    "        l1_results: the outputs of the attention heads at layer 1 (with head as one of the dimensions)\n",
    "        l2_results: the outputs of the attention heads at layer 2 (with head as one of the dimensions)\n",
    "        W_U: the unembedding matrix\n",
    "        tokens: the token ids of the sequence\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (seq_len-1, n_components)\n",
    "        represents the concatenation (along dim=-1) of logit attributions from:\n",
    "            the direct path (seq-1,1)\n",
    "            layer 0 logits (seq-1, n_heads)\n",
    "            layer 1 logits (seq-1, n_heads)\n",
    "        so n_components = 1 + 2*n_heads\n",
    "    '''\n",
    "    W_U_correct_tokens = W_U[:, tokens[1:]]  # (d_model, seq-1, n_components)\n",
    "    direct = einops.einsum(embed[:-1], W_U_correct_tokens, \"seq d_model, d_model seq -> seq\").unsqueeze(-1)\n",
    "    layer0 = einops.einsum(l1_results[:-1], W_U_correct_tokens, \"seq nheads d_model, d_model seq -> seq nheads\")\n",
    "    layer1 = einops.einsum(l2_results[:-1], W_U_correct_tokens, \"seq nheads d_model, d_model seq -> seq nheads\")\n",
    "    out = t.cat([direct, layer0, layer1], dim=-1)\n",
    "    return out\n",
    "\n",
    "\n",
    "#text = \"this is a very short text\"\n",
    "text = \"We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this.\"\n",
    "logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "str_tokens = model.to_str_tokens(text)\n",
    "tokens = model.to_tokens(text)\n",
    "\n",
    "with t.inference_mode():\n",
    "    embed = cache[\"embed\"]\n",
    "    l1_results = cache[\"result\", 0]\n",
    "    l2_results = cache[\"result\", 1]\n",
    "    logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, tokens[0])\n",
    "    # Uses fancy indexing to get a len(tokens[0])-1 length tensor, where the kth entry is the predicted logit for the correct k+1th token\n",
    "    correct_token_logits = logits[0, t.arange(len(tokens[0]) - 1), tokens[0, 1:]]\n",
    "    t.testing.assert_close(logit_attr.sum(1), correct_token_logits, atol=1e-3, rtol=0)\n",
    "    print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = cache[\"embed\"]\n",
    "l1_results = cache[\"result\", 0]\n",
    "l2_results = cache[\"result\", 1]\n",
    "logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, tokens[0])\n",
    "\n",
    "plot_logit_attribution(model, logit_attr, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "\n",
    "embed = rep_cache[\"embed\"]\n",
    "l1_results = rep_cache[\"result\", 0]\n",
    "l2_results = rep_cache[\"result\", 1]\n",
    "first_half_tokens = rep_tokens[0, : 1 + seq_len]\n",
    "second_half_tokens = rep_tokens[0, seq_len : ]\n",
    "\n",
    "first_half_logit_attr = logit_attribution(embed[: seq_len + 1], l1_results[: seq_len + 1], l2_results[: seq_len + 1], model.W_U, first_half_tokens)\n",
    "second_half_logit_attr = logit_attribution(embed[seq_len : ], l1_results[seq_len : ], l2_results[seq_len : ], model.W_U, second_half_tokens)\n",
    "assert first_half_logit_attr.shape == (seq_len, 2 * model.cfg.n_heads + 1)\n",
    "assert second_half_logit_attr.shape == (seq_len, 2 * model.cfg.n_heads + 1)\n",
    "\n",
    "plot_logit_attribution(model, first_half_logit_attr, first_half_tokens, \"Logit attr (first half)\")\n",
    "plot_logit_attribution(model, second_half_logit_attr, second_half_tokens, \"Logit attr (second half)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks: Intervening on Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_ablation_hook(\n",
    "    v: Float[Tensor, \"batch seq n_heads d_head\"],\n",
    "    hook: HookPoint,\n",
    "    head_index_to_ablate: int\n",
    ") -> Float[Tensor, \"batch seq n_heads d_head\"]:\n",
    "    v[:, :, head_index_to_ablate, :] = 0.\n",
    "    return v\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, tokens):\n",
    "    '''\n",
    "    Computes the mean cross entropy between logits (the model's prediction) and tokens (the true values).\n",
    "\n",
    "    (optional, you can just use return_type=\"loss\" instead.)\n",
    "    '''\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    pred_log_probs = t.gather(log_probs[:, :-1], -1, tokens[:, 1:, None])[..., 0]\n",
    "    return -pred_log_probs.mean()\n",
    "\n",
    "\n",
    "def get_ablation_scores(\n",
    "    model: HookedTransformer, \n",
    "    tokens: Int[Tensor, \"batch seq\"]\n",
    ") -> Float[Tensor, \"n_layers n_heads\"]:\n",
    "    '''\n",
    "    Returns a tensor of shape (n_layers, n_heads) containing the increase in cross entropy loss from ablating the output of each head.\n",
    "    '''\n",
    "    # Initialize an object to store the ablation scores\n",
    "    ablation_scores = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "\n",
    "    # Calculating loss without any ablation, to act as a baseline\n",
    "    model.reset_hooks()\n",
    "    logits = model(tokens, return_type=\"logits\")\n",
    "    seq_len = (tokens.shape[1] - 1) // 2\n",
    "    loss_no_ablation = cross_entropy_loss(logits[:, -seq_len:], tokens[:, -seq_len:])\n",
    "\n",
    "    for layer in tqdm(range(model.cfg.n_layers)):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            # Use functools.partial to create a temporary hook function with the head number fixed\n",
    "            temp_hook_fn = functools.partial(head_ablation_hook, head_index_to_ablate=head)\n",
    "            # Run the model with the ablation hook\n",
    "            ablated_logits = model.run_with_hooks(tokens, fwd_hooks=[\n",
    "                (utils.get_act_name(\"v\", layer), temp_hook_fn)\n",
    "            ])\n",
    "            # Calculate the loss difference\n",
    "            loss = cross_entropy_loss(ablated_logits[:, -seq_len:], tokens[:, -seq_len:])\n",
    "            # Store the result, subtracting the clean loss so that a value of zero means no change in loss\n",
    "            ablation_scores[layer, head] = loss - loss_no_ablation\n",
    "\n",
    "    return ablation_scores\n",
    "\n",
    "\n",
    "ablation_scores = get_ablation_scores(model, rep_tokens)\n",
    "tests.test_get_ablation_scores(ablation_scores, model, rep_tokens)\n",
    "\n",
    "imshow(\n",
    "    ablation_scores, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Loss diff\"},\n",
    "    title=\"Loss Difference After Ablating Heads\", \n",
    "    text_auto=\".2f\",\n",
    "    width=900, height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: different ablations\n",
    "\n",
    "### Ablating everything except induction capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def everything_but_prev_token_and_induction(name):\n",
    "    if utils.get_act_name(\"v\") in name:\n",
    "        if '0' in name:\n",
    "            for head in range(model.cfg.n_heads):\n",
    "                if head != 7:\n",
    "                    return True\n",
    "        if '1' in name:\n",
    "            for head in range(model.cfg.n_heads):\n",
    "                if head not in {4, 10}:\n",
    "                    return True\n",
    "\n",
    "very_ablated_logits = model.run_with_hooks(tokens, fwd_hooks=[(everything_but_prev_token_and_induction, zero_ablate_hook)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.get_act_name(\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Reverse-Engineering induction circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to this on October 7, 2024. Finished section 3 on September 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresher on induction circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: What is the interpretation of each of the following matrices?\n",
    "\n",
    "Describe (a) the *type of input* it takes, and (b) what the *output represents*.\n",
    "\n",
    "1. $W^h_{OV}$\n",
    "\n",
    "    a. Size: $d_{\\text{model}} \\times d_{\\text{model}}$\n",
    "\n",
    "    b. Input: $x$ in residual stream\n",
    "\n",
    "    b. Output: $x^\\top W^h_{OV}$ is the vector written to the residual stream at the **destination position**, if the destination token only pays attention to the source token at the position of $x$.\n",
    "\n",
    "2. $W_E W^h_{OV} W_U$\n",
    "\n",
    "    a. Size: $d_{voc} \\times d_{voc}$\n",
    "\n",
    "    b. Input: token $t$\n",
    "\n",
    "    c. Output: logit $t^\\top W_E W_{OV} W_U$. This is how much the token $t$ is upweighted at the destination position?\n",
    "\n",
    "3. $W_{QK}^h$\n",
    "\n",
    "    a. Size:  $d_{\\text{model}} \\times d_{\\text{model}}$\n",
    "\n",
    "    b. Input: $x$\n",
    "\n",
    "    c. Output: $x_i ^\\top W_Q W_K^\\top x_j$ is the attention score paid by token $i$ to token $j$\n",
    "\n",
    "4. $W_E W_{QK} W_E^\\top$ (and similarly $W_{\\text{pos}}$)\n",
    "\n",
    "    a. size: $d_{voc} \\times d_{voc}$ (or $n_{ctx} \\times n_{ctx}$)\n",
    "\n",
    "    b. input: token\n",
    "\n",
    "    c. output: $s^\\top [ - ] t$ is how much attention $t$ pays to $s$ -- tokens if $W_E$, positions in context if $W_{pos}$.\n",
    "\n",
    "5. $W_E W_{OV}^{h_1} W_{QK}^{h_2} W_E^\\top$\n",
    "\n",
    "    a. size: $d_{voc} \\times d_{voc}$\n",
    "\n",
    "    b. input: token\n",
    "\n",
    "    c. output: $s^\\top [ - ] t$ \n",
    "\n",
    "#### TODO: internalize what the heck this last one is doing, hold the whole thing in my head**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for section 4, so you don't need to run all of the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    d_model = 768,\n",
    "    d_head = 64,\n",
    "    n_heads = 12,\n",
    "    n_layers = 2,\n",
    "    n_ctx = 2048,\n",
    "    d_vocab = 50278,\n",
    "    attention_dir = 'causal',\n",
    "    attn_only = 'True',\n",
    "    tokenizer_name = 'EleutherAI/gpt-neox-20b',\n",
    "    seed = 398,\n",
    "    use_attn_result = True,\n",
    "    normalization_type = None,  # default would be 'LN', which is layernorm\n",
    "    positional_embedding_type = 'shortformer' # positional embedding only used for q and k, not for v? apparently makes induction heads more likely?\n",
    ")\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO_ID = \"callummcdougall/attn_only_2L_half\"\n",
    "FILENAME = \"attn_only_2L_half.pth\"\n",
    "\n",
    "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "\n",
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = t.load(weights_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_repeated_tokens(\n",
    "        model: HookedTransformer, seq_len: int, batch: int = 1\n",
    ") -> Int[Tensor, \"batch full_seq_len\"]:\n",
    "    \"\"\"\n",
    "    Generates a sequence of repeated random tokens.\n",
    "    Output is rep_tokens: [batch, 1 + 2*seq_len]\n",
    "    \"\"\"\n",
    "    d_vocab = model.cfg.d_vocab\n",
    "    prefix = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long()\n",
    "    rand_tokens = t.randint(0, d_vocab, size=(batch, seq_len))\n",
    "    rep_tokens = einops.repeat(rand_tokens, 'batch seq -> batch (2 seq)')\n",
    "    rep_tokens = t.cat([prefix, rep_tokens], dim=1)\n",
    "    return rep_tokens\n",
    "\n",
    "def run_and_cache_model_repeated_tokens(\n",
    "        model: HookedTransformer, seq_len: int, batch: int = 1\n",
    ") -> tuple[Tensor, Tensor, ActivationCache]:\n",
    "    \"\"\"\n",
    "    Generates a sequence of repeated random tokens, and runs the model on it, returning logits, tokens and cache\n",
    "\n",
    "    Should use the `generate_repeated_tokens` function above\n",
    "\n",
    "    Outputs are:\n",
    "        rep_tokens: [batch, 1+2*seq_len]\n",
    "        rep_logits: [batch, 1+2*seq_len, d_vocab]\n",
    "        rep_cache: The cache of the model run on rep_tokens\n",
    "    \"\"\"\n",
    "    tokens = generate_repeated_tokens(model, seq_len, batch).to(device)\n",
    "    logits, cache = model.run_with_cache(tokens, return_type='logits')\n",
    "    return tokens, logits, cache\n",
    "    \n",
    "seq_len = 50\n",
    "batch = 1\n",
    "(rep_tokens, rep_logits, rep_cache) = run_and_cache_model_repeated_tokens(model, seq_len, batch)\n",
    "rep_cache.remove_batch_dim()\n",
    "rep_str = model.to_str_tokens(rep_tokens)\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OV copying circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: compute OV circuit for `1.4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "head_index = 4\n",
    "\n",
    "full_OV_circuit = FactoredMatrix(model.W_E @ model.W_V[layer, head_index] , model.W_O[layer, head_index] @ model.W_U )\n",
    "tests.test_full_OV_circuit(full_OV_circuit, model, layer, head_index)\n",
    "\n",
    "indices = t.randint(full_OV_circuit.A.shape[0], size=(200,))\n",
    "full_OV_circuit_sample = full_OV_circuit[indices, indices].AB\n",
    "\n",
    "imshow(\n",
    "    full_OV_circuit_sample,\n",
    "    labels={\"x\": \"Input token\", \"y\": \"Logits on output token\"},\n",
    "    title=\"Full OV circuit for copying head\",\n",
    "    height=800, width=700,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_1_acc(full_OV_circuit: FactoredMatrix, batch_size: int = 1000) -> float:\n",
    "    '''\n",
    "    This should take the argmax of each column (ie over dim=0) and return the fraction of the time that's equal to the correct logit\n",
    "    '''\n",
    "    total = 0\n",
    "    for indices in t.split(t.arange(full_OV_circuit.shape[0]), batch_size):\n",
    "        AB_slice = full_OV_circuit[indices].AB\n",
    "        total += (t.argmax(AB_slice, dim=1) == indices.to(device)).float().sum().item()\n",
    "    return total / full_OV_circuit.shape[0]\n",
    "\n",
    "print(f\"Fraction of the time that the best logit is on the diagonal: {top_1_acc(full_OV_circuit):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_W_V = t.cat([model.W_V[1, 4], model.W_V[1, 10]], dim=1)\n",
    "cat_W_O = t.cat([model.W_O[1, 4], model.W_O[1, 10]], dim=0)\n",
    "\n",
    "effective_OV_circuit = FactoredMatrix(model.W_E @ cat_W_V , cat_W_O @ model.W_U)\n",
    "\n",
    "print(f\"Fraction of the time that the best logit is on the diagonal: {top_1_acc(effective_OV_circuit):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QK prev-token circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_scores(attn_scores: Float[Tensor, \"query_nctx key_nctx\"]):\n",
    "    '''Mask the attention scores so that tokens don't attend to previous tokens'''\n",
    "    assert attn_scores.shape == (model.cfg.n_ctx, model.cfg.n_ctx)\n",
    "    mask = t.tril(t.ones_like(attn_scores)).bool()\n",
    "    neg_inf = t.tensor(-1.0e6).to(attn_scores.device)\n",
    "    masked_attn_scores = t.where(mask, attn_scores, neg_inf)\n",
    "    return masked_attn_scores\n",
    "\n",
    "\n",
    "layer = 0\n",
    "head = 7\n",
    "\n",
    "W_pos = model.W_pos\n",
    "W_QK = model.W_Q[layer, head_index] @ model.W_K[layer, head_index].T\n",
    "pos_by_pos_scores = W_pos @ W_QK @ W_pos.T\n",
    "masked_scaled = mask_scores(pos_by_pos_scores / model.cfg.d_head ** 0.5)\n",
    "pos_by_pos_pattern = t.softmax(masked_scaled, dim=-1)\n",
    "tests.test_pos_by_pos_pattern(pos_by_pos_pattern, model, layer, head_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg first subdiagonal diagonal value: {pos_by_pos_pattern.diag(-1).mean():.4f}\")\n",
    "\n",
    "imshow(\n",
    "    utils.to_numpy(pos_by_pos_pattern[:100, :100]), \n",
    "    labels={\"x\": \"Key\", \"y\": \"Query\"}, \n",
    "    title=\"Attention patterns for prev-token QK circuit, first 100 indices\",\n",
    "    width=700\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-composition circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in rep_cache.keys():\n",
    "    print(list(rep_cache[key].shape),\"\\t\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_qk_input = decompose_qk_input(rep_cache)\n",
    "decomposed_qk_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_qk_input(cache: ActivationCache) -> t.Tensor:\n",
    "    '''\n",
    "    Output is decomposed_qk_input, with shape [2+num_heads, seq, d_model]\n",
    "                                (in this case [14, 101, 768])\n",
    "    The [i, :, :]th element is y_i (from notation above)\n",
    "    '''\n",
    "    embed = cache[\"embed\"]           # (seq, d_model)\n",
    "    pos_embed = cache[\"pos_embed\"]   # (seq, d_model)\n",
    "    l0_results = cache[\"result\", 0]  # (seq, heads, d_model)\n",
    "\n",
    "    embed = einops.rearrange(embed, \"seq dmodel -> seq 1 dmodel\")\n",
    "    pos_embed = einops.rearrange(pos_embed, \"seq dmodel -> seq 1 dmodel\")\n",
    "    decomposed_qk_input = t.cat([embed, pos_embed, l0_results], dim=1)\n",
    "                         \n",
    "    return einops.rearrange(decomposed_qk_input, \"seq heads dmodel -> heads seq dmodel\")\n",
    "    \n",
    "\n",
    "def decompose_q(\n",
    "    decomposed_qk_input: Float[Tensor, \"n_heads+2 posn d_head\"],\n",
    "    ind_head_index: int,\n",
    "    model: HookedTransformer,\n",
    ") -> Tensor:\n",
    "    '''\n",
    "    Output is decomposed_q with shape [2+num_heads, position, d_head]\n",
    "\n",
    "    The [i, :, :]th element is y_i @ W_Q (so the sum along axis 0 is just the q-values)\n",
    "    '''\n",
    "    W_Q = model.W_Q[1, ind_head_index]\n",
    "    return einops.einsum(decomposed_qk_input, W_Q, \"head seq dmodel, dmodel dhead -> head seq dhead\")\n",
    "\n",
    "\n",
    "def decompose_k(\n",
    "    decomposed_qk_input: Float[Tensor, \"n_heads+2 posn d_head\"],\n",
    "    ind_head_index: int,\n",
    "    model: HookedTransformer,\n",
    ") -> Tensor:\n",
    "    '''\n",
    "    Output is decomposed_k with shape [2+num_heads, position, d_head]\n",
    "\n",
    "    The [i, :, :]th element is y_i @ W_K (so the sum along axis 0 is just the k-values)\n",
    "    '''\n",
    "    W_K = model.W_K[1, ind_head_index]\n",
    "    return einops.einsum(decomposed_qk_input, W_K, \"head seq dmodel, dmodel dhead -> head seq dhead\")\n",
    "\n",
    "ind_head_index = 4\n",
    "# First we get decomposed q and k input, and check they're what we expect\n",
    "decomposed_qk_input = decompose_qk_input(rep_cache)\n",
    "decomposed_q = decompose_q(decomposed_qk_input, ind_head_index, model)\n",
    "decomposed_k = decompose_k(decomposed_qk_input, ind_head_index, model)\n",
    "t.testing.assert_close(decomposed_qk_input.sum(0), rep_cache[\"resid_pre\", 1] + rep_cache[\"pos_embed\"], rtol=0.01, atol=1e-05)\n",
    "t.testing.assert_close(decomposed_q.sum(0), rep_cache[\"q\", 1][:, ind_head_index], rtol=0.01, atol=0.001)\n",
    "t.testing.assert_close(decomposed_k.sum(0), rep_cache[\"k\", 1][:, ind_head_index], rtol=0.01, atol=0.01)\n",
    "# Second, we plot our results\n",
    "component_labels = [\"Embed\", \"PosEmbed\"] + [f\"0.{h}\" for h in range(model.cfg.n_heads)]\n",
    "for decomposed_input, name in [(decomposed_q, \"query\"), (decomposed_k, \"key\")]:\n",
    "    imshow(\n",
    "        utils.to_numpy(decomposed_input.pow(2).sum([-1])), \n",
    "        labels={\"x\": \"Position\", \"y\": \"Component\"},\n",
    "        title=f\"Norms of components of {name}\", \n",
    "        y=component_labels,\n",
    "        width=1000, height=400\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_attn_scores(decomposed_q: t.Tensor, decomposed_k: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    Output is decomposed_scores with shape [query_component, key_component, query_pos, key_pos]\n",
    "\n",
    "    The [i, j, :, :]th element is y_i @ W_QK @ y_j^T (so the sum along both first axes are the attention scores)\n",
    "    '''\n",
    "    d_head = 12\n",
    "    attn = einops.einsum(decomposed_q, decomposed_k, \"nQ seqQ dhead, nK seqK dhead -> nQ nK seqQ seqK\")\n",
    "    return attn #/ d_head ** 0.5\n",
    "\n",
    "tests.test_decompose_attn_scores(decompose_attn_scores, decomposed_q, decomposed_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_scores = decompose_attn_scores(decomposed_q, decomposed_k)\n",
    "decomposed_stds = einops.reduce(\n",
    "    decomposed_scores, \n",
    "    \"query_decomp key_decomp query_pos key_pos -> query_decomp key_decomp\", \n",
    "    t.std\n",
    ")\n",
    "\n",
    "# First plot: attention score contribution from (query_component, key_component) = (Embed, L0H7)\n",
    "imshow(\n",
    "    utils.to_numpy(t.tril(decomposed_scores[0, 9])), \n",
    "    title=\"Attention score contributions from (query, key) = (embed, output of L0H7)\",\n",
    "    width=800\n",
    ")\n",
    "\n",
    "# Second plot: std dev over query and key positions, shown by component\n",
    "imshow(\n",
    "    utils.to_numpy(decomposed_stds), \n",
    "    labels={\"x\": \"Key Component\", \"y\": \"Query Component\"},\n",
    "    title=\"Standard deviations of attention score contributions (by key and query component)\", \n",
    "    x=component_labels, \n",
    "    y=component_labels,\n",
    "    width=800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find full K-composition circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_K_comp_full_circuit(\n",
    "    model: HookedTransformer,\n",
    "    prev_token_head_index: int,\n",
    "    ind_head_index: int\n",
    ") -> FactoredMatrix:\n",
    "    '''\n",
    "    Returns a (vocab, vocab)-size FactoredMatrix, with the first dimension being the query side \n",
    "    and the second dimension being the key side (going via the previous token head)\n",
    "    '''\n",
    "    W_E = model.W_E\n",
    "    W_QK = FactoredMatrix(model.W_Q[1, ind_head_index], model.W_K[1, ind_head_index].T)\n",
    "    W_OV = FactoredMatrix(model.W_V[0, prev_token_head_index], model.W_O[0, prev_token_head_index])\n",
    "    return W_E @ W_QK @ W_OV.T @ W_E.T\n",
    "\n",
    "\n",
    "prev_token_head_index = 7\n",
    "ind_head_index = 4\n",
    "K_comp_circuit = find_K_comp_full_circuit(model, prev_token_head_index, ind_head_index)\n",
    "\n",
    "tests.test_find_K_comp_full_circuit(find_K_comp_full_circuit, model)\n",
    "\n",
    "print(f\"Fraction of tokens where the highest activating key is the same token: {top_1_acc(K_comp_circuit.T):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
